{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<style>\n",
      "@font-face {\n",
      "  font-family: CharisSILW;\n",
      "  src: url(files/CharisSIL-R.woff);\n",
      "}\n",
      "@font-face {\n",
      "  font-family: CharisSILW;\n",
      "  font-style: italic;\n",
      "  src: url(files/CharisSIL-I.woff);\n",
      "}\n",
      "@font-face {\n",
      "\tfont-family: CharisSILW;\n",
      "\tfont-weight: bold;\n",
      "\tsrc: url(files/CharisSIL-B.woff);\n",
      "}\n",
      "@font-face {\n",
      "\tfont-family: CharisSILW;\n",
      "\tfont-weight: bold;\n",
      "\tfont-style: italic;\n",
      "\tsrc: url(files/CharisSIL-BI.woff);\n",
      "}\n",
      "\n",
      "div.cell, div.text_cell_render{\n",
      "    max-width:750px;\n",
      "    margin-left:auto;\n",
      "    margin-right:auto;\n",
      "}\n",
      "\n",
      "h1 {\n",
      "    text-align:center;\n",
      "    font-family: Charis SIL, CharisSILW, serif;\n",
      "}\n",
      "\n",
      ".rendered_html {\n",
      "    font-size: 130%;\n",
      "    line-height: 1.3;\n",
      "}\n",
      "\n",
      ".rendered_html li {\n",
      "    line-height: 2;\n",
      "}\n",
      "\n",
      ".rendered_html h1{\n",
      "    line-height: 1.3;\n",
      "}\n",
      "\n",
      ".rendered_html h2{\n",
      "    line-height: 1.2;\n",
      "}\n",
      "\n",
      ".rendered_html h3{\n",
      "    line-height: 1.0;\n",
      "}\n",
      "\n",
      ".text_cell_render {\n",
      "    font-family: Charis SIL, CharisSILW, serif;\n",
      "    line-height: 145%;\n",
      "}\n",
      "\n",
      "li li {\n",
      "    font-size: 85%;\n",
      "}\n",
      "</style>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Scikit-Learn: Machine Learning in the Python ecosystem\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<center>\n",
      "<img src=\"files/sklearn-logo.png\" /><br />\n",
      "MLOSS workshop - NIPS 2013<br />\n",
      "December 10, 2013<br />\n",
      "<br />\n",
      "Gilles Louppe ([@glouppe](https://twitter.com/glouppe))<br />\n",
      "<br />\n",
      "<small><em>(with Ga\u00ebl Varoquaux ([@GaelVaroquaux](https://twitter.com/GaelVaroquaux)) <br />and Andreas M\u00fcller ([@t3kcit](https://twitter.com/t3kcit)) as backups)</em></small>\n",
      "</center>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Scikit-Learn\n",
      "\n",
      "* Machine learning library written in __Python__\n",
      "* __Simple and efficient__, for both experts and non-experts\n",
      "* Classical, __well-established machine learning algorithms__\n",
      "* __BSD 3 license__"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Overview\n",
      "\n",
      "__Supervised learning:__\n",
      "\n",
      "* Linear models (Ridge, Lasso, Elastic Net, ...)\n",
      "* Ensemble methods (Random Forests, Bagging, GBRT, ...)\n",
      "* Support Vector Machines\n",
      "\n",
      "<center><img src=\"files/comparison.png\" />\n",
      "<em>A comparison of (some of the) classifiers in Scikit-Learn</em></center><br />\n",
      "\n",
      "__Unsupervised learning:__\n",
      "\n",
      "* Clustering (KMeans, Ward, ...)\n",
      "* Matrix decomposition (PCA, ICA, ...)\n",
      "* Outlier detection\n",
      "\n",
      "__Model selection and evaluation:__\n",
      "\n",
      "* Cross-validation\n",
      "* Grid-search\n",
      "* Lots of metrics\n",
      "\n",
      "_... and many more!_ (See our [Reference](http://scikit-learn.org/stable/modules/classes.html))"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Efficiency\n",
      "\n",
      "* Batch processing using NumPy and SciPy operations\n",
      "* Critical code is written in Cython\n",
      "* Parallelism"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Collaborative development\n",
      "\n",
      "- 10~ core developers (mostly researchers)\n",
      "- 100+ occasional contributors\n",
      "- __All working together__ on [GitHub](https://github.com/scikit-learn/scikit-learn)\n",
      "- Emphasis on __keeping the project maintainable__\n",
      "    - Style consistency\n",
      "    - Unit-test coverage\n",
      "    - Documentation and examples\n",
      "    - Code review\n",
      "    \n",
      "__Number of commits:__\n",
      "<center><br /><img src=\"files/commits.png\" /></center>\n",
      "\n",
      "<br />\n",
      "\n",
      "__Lines of code:__\n",
      "<center><br /><img src=\"files/loc.png\" /></center>\n",
      "\n",
      "(Graphs generated by [Ohloh.net](http://www.ohloh.net/p/scikit-learn))"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## A simple and unified API\n",
      "\n",
      "All objects in scikit-learn share a uniform and limited API consisting of three complementary interfaces:\n",
      "\n",
      "- an `estimator` interface for building and fitting models;\n",
      "- a `predictor` interface for making predictions;\n",
      "- a `transformer` interface for converting data.\n",
      "\n",
      "All of them takes as input __data which is structured as Numpy arrays__. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Load data\n",
      "from sklearn.datasets import load_digits\n",
      "data = load_digits()\n",
      "X, y = data.data, data.target\n",
      "\n",
      "print type(X)\n",
      "print X"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Consistency across the package makes scikit-learn very usable in practice. Experimenting with different learning algorithms is as simple as changing a class definition."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Instantiate a classifier\n",
      "from sklearn.tree import DecisionTreeClassifier # Change this\n",
      "clf = DecisionTreeClassifier()                  # ... and that\n",
      "\n",
      "# Do stuff (and rely on the common interface across estimators)\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.cross_validation import cross_val_score\n",
      "\n",
      "pipeline = Pipeline([(\"pca\", PCA(n_components=10)), \n",
      "                     (\"classifier\", clf)])\n",
      "\n",
      "print \"Accuracy =\", cross_val_score(pipeline, X, y, cv=3).mean()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Through __composition interfaces__ (e.g., `Pipeline` and `FeatureUnion`), the library also offers powerful mechanisms to express a wide variety of learning tasks within a small amount of code.\n",
      "\n",
      "Finally, through duck-typing, the consistent API leads to a library that is __easily extensible__. It allows user-defined estimators to be easily incorporated into the Scikit-Learn workflow without any explicit object inheritance."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Integration in the scientific Python ecosystem\n",
      "\n",
      "- The __open source__ Python ecosystem provides __a standalone, versatile and powerful scientific working environment__, including:\n",
      "     - [NumPy](http://numpy.org) (for efficient manipulation of multi-dimensional arrays);\n",
      "     - [SciPy](http://scipy.org) (for specialized data structures (e.g., sparse matrices) and lower-level scientific algorithms), \n",
      "     - [IPython](http://ipython.org) (for interactive exploration),\n",
      "     - [Matplotlib](http://matplotlib.org) (for vizualization) \n",
      "     - [Pandas](http://pandas.pydata.org/) (for data management and data analysis) \n",
      "     - _(and many others...)_\n",
      "- Scikit-Learn builds upon NumPy and SciPy and __complements__ this scientific environment with machine learning algorithms;\n",
      "- By design, Scikit-Learn is __non-intrusive__, easy to use and easy to combine with other libraries.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## End-to-end proof of concept\n",
      "\n",
      "Let us illustrate the various components of the scientific Python ecosystem for the analysis of scientific data. \n",
      "\n",
      "We consider genetic data from the [HapMap project](http://hapmap.ncbi.nlm.nih.gov/) which catalogs common genetic variants in human beings from different populations in different parts of the world. \n",
      "\n",
      "> _Genetics 101._ The DNA in our cells are made of long chains of 4 chemical building blocks (adenine, thymine, cytosine, and guanine) which are strung together in 23 pairs of chromosomes. These genetic sequences contain information that influences our physical traits, our likelihood of suffering from disease, or the responses of our bodies to substances that we encounter in the environment.\n",
      "\n",
      "> The genetic sequences of different people are remarkably similar. When the chromosomes of two humans are compared, their DNA sequences can be identical for hundreds of bases. But at about one in every 1200 bases, on average, the sequences differ. These genetic differences are known as single nucleotide polymorphisms (_SNPs_). By identifying most of the approximately 10 million SNPs estimated to occur commonly in the human genome, the HapMap Project is identifying the basis for a large fraction of the genetic diversity in the human species.\n",
      "\n",
      "<center>\n",
      "<img src=\"files/snp.png\" /><br />\n",
      "<em>DNA sequences 1 and 2 are similar to each other,<br /> \n",
      "except at some specific locations called _SNPs_.</em>\n",
      "</center><br />\n",
      "\n",
      "The HapMap data includes genotypes for 1417 individuals, from 11 human populations from different parts of the world (e.g., Europe, Africa, Asia; see [Appendix A](#appendix-populations)). It regroups genotypes for all SNPs (from all 23 chromosomes) reported by the HapMap genotyping centers. \n",
      "\n",
      "From a data management point of view, the HashMap data comes with many of the hurdles of real-world data:\n",
      "\n",
      "- It is scattered in [a large collection](http://hapmap.ncbi.nlm.nih.gov/downloads/genotypes/2010-08_phaseII+III/forward/) of genotype data files (one per chromosome/population);\n",
      "- It is large (1417 individuals, millions of SNPs; GBs of raw data);\n",
      "- SNPs are not all reported for all populations;\n",
      "- Genotypes are sometimes missing;\n",
      "- Values are heterogeneous (numbers, strings, symbols). \n",
      "\n",
      "We will explore and analyze the HapMap data directly from this IPython session.\n",
      "\n",
      "> `IPython` provides an __interactive web-based working environment__ (called _Notebook_) combining code execution, text, plots and media into a single document."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### a) Downloading the data \n",
      "\n",
      "Besides Python code, IPython also allows for the execution of arbitrary `bash` commands, which extends the Python ecosystem with all tools available from the command line. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "# Load the data from chromosome 15\n",
      "mkdir -p data\n",
      "wget -P data http://hapmap.ncbi.nlm.nih.gov/downloads/genotypes/2010-08_phaseII+III/forward/genotypes_chr15_ASW_r28_nr.b36_fwd.txt.gz\n",
      "wget -P data http://hapmap.ncbi.nlm.nih.gov/downloads/genotypes/2010-08_phaseII+III/forward/genotypes_chr15_CEU_r28_nr.b36_fwd.txt.gz\n",
      "wget -P data http://hapmap.ncbi.nlm.nih.gov/downloads/genotypes/2010-08_phaseII+III/forward/genotypes_chr15_CHB_r28_nr.b36_fwd.txt.gz\n",
      "wget -P data http://hapmap.ncbi.nlm.nih.gov/downloads/genotypes/2010-08_phaseII+III/forward/genotypes_chr15_CHD_r28_nr.b36_fwd.txt.gz\n",
      "wget -P data http://hapmap.ncbi.nlm.nih.gov/downloads/genotypes/2010-08_phaseII+III/forward/genotypes_chr15_GIH_r28_nr.b36_fwd.txt.gz\n",
      "wget -P data http://hapmap.ncbi.nlm.nih.gov/downloads/genotypes/2010-08_phaseII+III/forward/genotypes_chr15_JPT_r28_nr.b36_fwd.txt.gz\n",
      "wget -P data http://hapmap.ncbi.nlm.nih.gov/downloads/genotypes/2010-08_phaseII+III/forward/genotypes_chr15_LWK_r28_nr.b36_fwd.txt.gz\n",
      "wget -P data http://hapmap.ncbi.nlm.nih.gov/downloads/genotypes/2010-08_phaseII+III/forward/genotypes_chr15_MEX_r28_nr.b36_fwd.txt.gz\n",
      "wget -P data http://hapmap.ncbi.nlm.nih.gov/downloads/genotypes/2010-08_phaseII+III/forward/genotypes_chr15_MKK_r28_nr.b36_fwd.txt.gz\n",
      "wget -P data http://hapmap.ncbi.nlm.nih.gov/downloads/genotypes/2010-08_phaseII+III/forward/genotypes_chr15_TSI_r28_nr.b36_fwd.txt.gz\n",
      "wget -P data http://hapmap.ncbi.nlm.nih.gov/downloads/genotypes/2010-08_phaseII+III/forward/genotypes_chr15_YRI_r28_nr.b36_fwd.txt.gz\n",
      "for f in data/*.gz; do gunzip $f; done"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Look at the 3 first lines of the data \n",
      "!head -n 3 data/genotypes_chr15_ASW_r28_nr.b36_fwd.txt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### b) Data preprocessing with `pandas` and `sklearn`\n",
      "\n",
      "Scikit-Learn expects input __data__ to be represented __as two-dimensional NumPy arrays of numerical values__. In our case, the HapMap data is not represented in this way and needs to be preprocessed to conform to our standards.\n",
      "\n",
      "We rely on the `pandas` module for loading, converting and merging the datafiles into a single NumPy array. \n",
      "\n",
      "> The `pandas` module aims at providing fast, flexible and expressive data structures designed to make working with labeled or relational data both easy and intuitive. It is one of the most powerful and flexible Python packages for data manipulation. In particular, `pandas` integrates into the scientific Python ecosystem by providing NumPy-compatible data structures and tools for easily converting data from one format to another."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "def load_genotypes(filename, population):\n",
      "    # Load the data into a dataframe\n",
      "    df = pd.read_csv(filename, sep=\" \", index_col=0)\n",
      "\n",
      "    # Map genotypes to 0-1-2 values\n",
      "    mask = df.alleles == 'A/C'\n",
      "    df[mask] = df[mask].replace(to_replace=['AA', 'AC', 'CC'], \n",
      "                                value=[0, 1, 2])\n",
      "    mask = df.alleles == 'A/G'\n",
      "    df[mask] = df[mask].replace(to_replace=['AA', 'AG', 'GG'], \n",
      "                                value=[0, 1, 2])\n",
      "    mask = df.alleles == 'A/T'\n",
      "    df[mask] = df[mask].replace(to_replace=['AA', 'AT', 'TT'], \n",
      "                                value=[0, 1, 2])\n",
      "    mask = df.alleles == 'C/G'\n",
      "    df[mask] = df[mask].replace(to_replace=['CC', 'CG', 'GG'], \n",
      "                                value=[0, 1, 2])\n",
      "    mask = df.alleles == 'C/T'\n",
      "    df[mask] = df[mask].replace(to_replace=['CC', 'CT', 'TT'], \n",
      "                                value=[0, 1, 2])\n",
      "    mask = df.alleles == 'G/T'\n",
      "    df[mask] = df[mask].replace(to_replace=['GG', 'GT', 'TT'], \n",
      "                                value=[0, 1, 2])\n",
      "    \n",
      "    # Replace missing value placeholders\n",
      "    df.replace(to_replace='NN', value=-1, inplace=True)\n",
      "\n",
      "    # Copy data into a new frame\n",
      "    individuals = df.columns[10:]\n",
      "    snps = df.index\n",
      "\n",
      "    new_df = pd.DataFrame(index=individuals)\n",
      "    new_df[\"population\"] = population\n",
      "    new_df[snps] = df[individuals].T\n",
      "    new_df = new_df.convert_objects()\n",
      "\n",
      "    # Drop SNPs corresponding to deletions/insertions\n",
      "    new_df = new_df.drop(new_df.columns[new_df.dtypes == \"object\"][1:], axis=1)\n",
      "\n",
      "    return new_df\n",
      "\n",
      "# Concatenate all files and only keep common SNPs\n",
      "filename = \"genotypes_chr15_%s_r28_nr.b36_fwd.txt\"\n",
      "populations = [\"ASW\", \"CEU\", \"CHB\", \"CHD\", \"GIH\", \"JPT\", \n",
      "               \"LWK\", \"MEX\", \"MKK\", \"TSI\", \"YRI\"]\n",
      "\n",
      "df = pd.concat([load_genotypes(filename % population, population) \n",
      "                    for population in populations], \n",
      "               join=\"inner\")\n",
      "\n",
      "# Convert to Numpy arrays\n",
      "snps = df.columns[1:]\n",
      "X = df[snps].values.astype(np.int8)\n",
      "y = df[\"population\"].values.astype(np.str)\n",
      "\n",
      "# Save for later\n",
      "np.savez(\"data/chr15-all-numpy.npz\", X=X, y=y, snps=snps)\n",
      "\n",
      "# Warning! This takes long..."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# To save time, let us load the binary files generated above\n",
      "import numpy as np\n",
      "\n",
      "data = np.load(\"data/chr15-all-numpy.npz\")\n",
      "snps = data[\"snps\"]\n",
      "X = data[\"X\"]\n",
      "y = data[\"y\"]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"SNPs =\", snps[:10]\n",
      "print \"X =\", X\n",
      "print \"X.shape =\", X.shape\n",
      "print \"y =\", y"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Limit to 5000 SNPs for faster interactions\n",
      "snps = snps[:5000]\n",
      "X = X[:, :5000]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Data is now converted to NumPy arrays. However, it still contains missing values (marked as `-1`) that need to be filled before being fed to Scikit-Learn estimators.\n",
      "\n",
      "As a last preprocessing step, we rely on tools from the `sklearn.preprocessing` submodule to replace missing values. \n",
      "\n",
      "> The `sklearn.preprocessing` submodule includes algorithms for encoding, normalizing or imputing data. All are offered as transformers."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Before imputation:\", np.unique(X)\n",
      "\n",
      "from sklearn.preprocessing import Imputer\n",
      "imputer = Imputer(missing_values=-1, strategy=\"most_frequent\")\n",
      "X = imputer.fit_transform(X)\n",
      "\n",
      "print \"After imputation:\", np.unique(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### c) Data visualization with `matplotlib` \n",
      "\n",
      "In exploratory analysis, data visualization plays an important role in identifying interesting patterns. Representing your data graphically allows to visualize quantities and relationships that are otherwise difficult to interpret. In particular, when tackling a new problem, visualization often helps to quickly grasp what is going on in the data.\n",
      "\n",
      "We rely on the `matplotlib` module for generating our figures and plots.\n",
      "\n",
      "> The `matplotlib` module is a plotting library for producing high-quality figures with fine-grained layout control. It integrates into the IPython shell (\u00e0 la MATLAB) to provide an interactive environement."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Project the data to a 2D space for visualization\n",
      "from sklearn.decomposition import RandomizedPCA\n",
      "Xp = RandomizedPCA(n_components=2, random_state=1).fit_transform(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Setup matplotlib to work interactively\n",
      "%matplotlib inline\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "# Plot individuals \n",
      "populations = np.unique(y)\n",
      "colors = plt.get_cmap(\"hsv\")\n",
      "plt.figure(figsize=(10, 4))\n",
      "\n",
      "for i, p in enumerate(populations):\n",
      "    mask = (y == p)\n",
      "    plt.scatter(Xp[mask, 0], Xp[mask, 1], \n",
      "                c=colors(1. * i / 11), label=p)\n",
      "    \n",
      "plt.xlim([-30, 50])\n",
      "plt.legend(loc=\"best\")\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this plot, we observe that individuals from a same population are clustered together, which confirms that they share the same genetic variants. \n",
      "\n",
      "We also observe that some populations are grouped together into larger clusters (Asian people (CHN, CHD, JPT); African people (MKK, LWK, YRI); Caucasian people (CEU, TSI)), which suggests that individuals from those populations share common ancestors."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### d) Learn from the data with `sklearn`\n",
      "\n",
      "We now consider the learning task that consists in predicting the population of an individual given his genotype.\n",
      "\n",
      "Let us first learn a simple linear model."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import train_test_split\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
      "\n",
      "from sklearn.linear_model import RidgeClassifierCV\n",
      "clf = RidgeClassifierCV().fit(X_train, y_train)\n",
      "clf.score(X_test, y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics import confusion_matrix\n",
      "\n",
      "def print_cm(cm, labels):\n",
      "    # Nicely print the confusion matrix\n",
      "    print \" \" * 4,\n",
      "    for label in labels:\n",
      "        print \" %s\" % label,\n",
      "    print\n",
      "    \n",
      "    for i, label1 in enumerate(labels):\n",
      "        print label1,\n",
      "        for j, label2 in enumerate(labels):\n",
      "            print \"%4d\" % cm[i, j],\n",
      "        print\n",
      "    \n",
      "print_cm(confusion_matrix(y_test, \n",
      "                          clf.predict(X_test), \n",
      "                          labels=populations), populations)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "By examining the confusion matrix, we observe that errors mainly happen within the larger clusters previously highlighted. For example, people from Tuscany (TSI) are often wrongly predicted as Utah residents (CEU) - both indeed have European ancestors and were clustered together - but are never as people from Asia. \n",
      "\n",
      "Let us now examine the coefficients of the learned model."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Plot coefficients\n",
      "coef = np.mean(np.abs(clf.coef_), axis=0)\n",
      "\n",
      "plt.figure(figsize=(10, 4))\n",
      "plt.bar(range(len(snps)), coef)\n",
      "plt.show()\n",
      "\n",
      "# Top 10 SNPs\n",
      "indices = np.argsort(coef)[::-1]\n",
      "\n",
      "for i in range(10):\n",
      "    print coef[indices[i]], snps[indices[i]]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As indicated by dbSNP, [rs6497292](http://www.ncbi.nlm.nih.gov/projects/SNP/snp_ref.cgi?rs=6497292) happens to be located in gene [HERC2 (8924)](http://www.ncbi.nlm.nih.gov/gene?cmd=Retrieve&dopt=Graphics&list_uids=8924), known to be associated with skin/hair/eye pigmentation variability. It is therefore not surprising to find this SNP at the top of the ranking.\n",
      "\n",
      "The same kind of analysis can be carried out with other models from Scikit-Learn. For example, using a forest of randomized trees yields the following results:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "clf = ExtraTreesClassifier(n_estimators=100, \n",
      "                           max_features=0.2, \n",
      "                           n_jobs=2,\n",
      "                           random_state=1).fit(X_train, y_train)\n",
      "clf.score(X_test, y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print_cm(confusion_matrix(y_test, \n",
      "                          clf.predict(X_test), \n",
      "                          labels=populations), populations)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Plot importances\n",
      "importances = clf.feature_importances_\n",
      "\n",
      "plt.figure(figsize=(10, 4))\n",
      "plt.bar(range(len(snps)), importances)\n",
      "plt.show()\n",
      "\n",
      "# Top 10 SNPs\n",
      "indices = np.argsort(importances)[::-1]\n",
      "\n",
      "for i in range(10):\n",
      "    print importances[indices[i]], snps[indices[i]]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Results are not the same as previously. However SNPs that are reported are still meaningful: [rs1800410](http://www.ncbi.nlm.nih.gov/projects/SNP/snp_ref.cgi?rs=1800410) is located within gene [OCA2 (4948)](http://www.ncbi.nlm.nih.gov/gene?cmd=Retrieve&dopt=Graphics&list_uids=4948), known to be associated with oculocutaneous (eyes and skin) albinism.\n",
      "\n",
      "Overall, this suggests that there is no single SNP responsible alone for how populations look. Instead, and as expected, traits are dilluted all along the genome and there exist multiple genetic variants that can be used to identify an individual."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Conclusions\n",
      "\n",
      "Python data-oriented packages tend to complement and integrate smoothly with each other. Together, they provide __a powerful, flexible and coherent working environment__ for real-world scientific data analysis.\n",
      "\n",
      "__Scikit-Learn complements this ecosystem with machine learning algorithms__ and data analysis utilities."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a name=\"appendix-populations\"></a>\n",
      "\n",
      "## Appendix A: Populations from the HapMap data\n",
      "\n",
      "- ASW: African ancestry in Southwest USA\n",
      "- CEU: Utah residents with Northern and Western European ancestry from the CEPH collection\n",
      "- CHB: Han Chinese in Beijing, China\n",
      "- CHD: Chinese in Metropolitan Denver, Colorado\n",
      "- GIH: Gujarati Indians in Houston, Texas\n",
      "- JPT: Japanese in Tokyo, Japan\n",
      "- LWK: Luhya in Webuye, Kenya\n",
      "- MEX: Mexican ancestry in Los Angeles, California\n",
      "- MKK: Maasai in Kinyawa, Kenya\n",
      "- TSI: Tuscan in Italy\n",
      "- YRI: Yoruban in Ibadan, Nigeria (West Africa)"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}
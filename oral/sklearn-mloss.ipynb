{
 "metadata": {
  "css": [
   ""
  ],
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Scikit-Learn: Machine Learning in the Python ecosystem\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<center>\n",
      "<img src=\"files/sklearn-logo.png\" /><br />\n",
      "MLOSS workshop - NIPS 2013<br />\n",
      "December 10, 2013<br />\n",
      "<br />\n",
      "Gilles Louppe ([@glouppe](https://twitter.com/glouppe))<br />\n",
      "<br />\n",
      "<small><em>(with Ga\u00ebl Varoquaux ([@GaelVaroquaux](https://twitter.com/GaelVaroquaux)) <br />and Andreas M\u00fcller ([@t3kcit](https://twitter.com/t3kcit)) as backups)</em></small>\n",
      "</center>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Scikit-Learn\n",
      "\n",
      "* Machine learning library written in __Python__\n",
      "* __Simple and efficient__, for both experts and non-experts\n",
      "* Classical, __well-established machine learning algorithms__\n",
      "    - Supervised and unsupervised algorithms\n",
      "    - Model selection and model evaluation\n",
      "    - Data preprocessing and feature engineering\n",
      "* __BSD 3 license__"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Collaborative development\n",
      "\n",
      "- 10~ core developers (mostly researchers)\n",
      "- 100+ occasional contributors\n",
      "- __All working together__ on [GitHub](https://github.com/scikit-learn/scikit-learn)\n",
      "- Emphasis on __keeping the project maintainable__\n",
      "    - Style consistency\n",
      "    - Unit-test coverage\n",
      "    - Documentation and examples\n",
      "    - Code review"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## A simple and unified API\n",
      "\n",
      "All objects in scikit-learn share a uniform and limited API consisting of three complementary interfaces:\n",
      "\n",
      "- an `estimator` interface for building and fitting models;\n",
      "- a `predictor` interface for making predictions;\n",
      "- a `transformer` interface for converting data.\n",
      "\n",
      "Consistency across the package makes scikit-learn very usable in practice. Experimenting with different learning algorithms is as simple as changing a class definition."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import cross_val_score\n",
      "from sklearn.datasets import load_iris\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "\n",
      "data = load_iris()\n",
      "X, y = data.data, data.target\n",
      "\n",
      "clf = DecisionTreeClassifier() # Change this\n",
      "\n",
      "print \"Accuracy =\", cross_val_score(clf, X, y, cv=3).mean()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Through composition interfaces (e.g., `Pipeline` and `FeatureUnion`), the library also offers powerful mechanisms to express a wide variety of learning tasks within a small amount of code.\n",
      "\n",
      "Through duck-typing, the consistent API leads to a library that is easily extensible. It allows user-defined estimators to be easily incorporated into the Scikit-Learn workflow without any explicit object inheritance."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Integration in the scientific Python ecosystem\n",
      "\n",
      "- The Python ecosystem provides __a standalone, versatile and powerful scientific working environment__, including:\n",
      "     - [NumPy](http://numpy.org) (for efficient manipulation of multi-dimensional arrays);\n",
      "     - [SciPy](http://scipy.org) (for specialized data structures (e.g., sparse matrices) and lower-level scientific algorithms), \n",
      "     - [IPython](http://ipython.org) (for interactive exploration),\n",
      "     - [Matplotlib](http://matplotlib.org) (for vizualization) \n",
      "     - [Pandas](http://pandas.pydata.org/) (for data management and data analysis) \n",
      "- Scikit-Learn builds upon NumPy and SciPy and __complements__ this scientific environment with machine learning algorithms routines;\n",
      "- By design, it is non-intrusive, easy to use and easy to combine with other libraries.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## End-to-end proof of concept\n",
      "\n",
      "Let us illustrate the various components of the scientific Python ecosystem for the analysis of scientific data. \n",
      "\n",
      "We consider genetic data from the [HapMap project](http://hapmap.ncbi.nlm.nih.gov/) which catalogs common genetic variants in human beings from different populations in different parts of the world. \n",
      "\n",
      "> _Genetics 101._ The DNA in our cells are made of long chains of 4 chemical building blocks (adenine, thymine, cytosine, and guanine) which are strung together in 23 pairs of chromosomes. These genetic sequences contain information that influences our physical traits, our likelihood of suffering from disease, and the responses of our bodies to substances that we encounter in the environment.\n",
      "\n",
      "> The genetic sequences of different people are remarkably similar. When the chromosomes of two humans are compared, their DNA sequences can be identical for hundreds of bases. But at about one in every 1,200 bases, on average, the sequences differ. These genetic differences are known as single nucleotide polymorphisms (_SNPs_). By identifying most of the approximately 10 million SNPs estimated to occur commonly in the human genome, the HapMap Project is identifying the basis for a large fraction of the genetic diversity in the human species.\n",
      "\n",
      "<center>\n",
      "<img src=\"files/snp.png\" /><br />\n",
      "<em>DNA sequences 1 and 2 are similar to each other,<br /> \n",
      "except at some specific locations called _SNPs_.</em>\n",
      "</center><br />\n",
      "\n",
      "In this dataset, SNPs are sampled from the following 11 populations:\n",
      "\n",
      "* ASW (A): African ancestry in Southwest USA\n",
      "* CEU (C): Utah residents with Northern and Western European ancestry from the CEPH collection\n",
      "* CHB (H): Han Chinese in Beijing, China\n",
      "* CHD (D): Chinese in Metropolitan Denver, Colorado\n",
      "* GIH (G): Gujarati Indians in Houston, Texas\n",
      "* JPT (J): Japanese in Tokyo, Japan\n",
      "* LWK (L): Luhya in Webuye, Kenya\n",
      "* MEX (M): Mexican ancestry in Los Angeles, California\n",
      "* MKK (K): Maasai in Kinyawa, Kenya\n",
      "* TSI (T): Tuscan in Italy\n",
      "* YRI (Y): Yoruban in Ibadan, Nigeria (West Africa)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### a) Setup the `IPython` session\n",
      "\n",
      "IPython provides an __interactive web-based working environment__ combining code execution, text, plots and media into a single document. We will explore and analyze the HapMap data directly into this IPython session. \n",
      "\n",
      "For a smoother integration of the Python ecosystem, IPython provides shortcuts for loading NumPy and Matplotlib and working interactively."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Shortcut for loading Numpy and Matplotlib\n",
      "%pylab --no-import-all inline "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### b) Download the data \n",
      "\n",
      "[TODO]: Explain that bash commands can be executed, comment on the file format."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Load the data from chromosome 15\n",
      "!mkdir -p data\n",
      "!wget -P data http://hapmap.ncbi.nlm.nih.gov/downloads/genotypes/2010-08_phaseII+III/forward/genotypes_chr15_ASW_r28_nr.b36_fwd.txt.gz\n",
      "!wget -P data http://hapmap.ncbi.nlm.nih.gov/downloads/genotypes/2010-08_phaseII+III/forward/genotypes_chr15_CEU_r28_nr.b36_fwd.txt.gz\n",
      "!wget -P data http://hapmap.ncbi.nlm.nih.gov/downloads/genotypes/2010-08_phaseII+III/forward/genotypes_chr15_CHB_r28_nr.b36_fwd.txt.gz\n",
      "!wget -P data http://hapmap.ncbi.nlm.nih.gov/downloads/genotypes/2010-08_phaseII+III/forward/genotypes_chr15_CHD_r28_nr.b36_fwd.txt.gz\n",
      "!wget -P data http://hapmap.ncbi.nlm.nih.gov/downloads/genotypes/2010-08_phaseII+III/forward/genotypes_chr15_GIH_r28_nr.b36_fwd.txt.gz\n",
      "!wget -P data http://hapmap.ncbi.nlm.nih.gov/downloads/genotypes/2010-08_phaseII+III/forward/genotypes_chr15_JPT_r28_nr.b36_fwd.txt.gz\n",
      "!wget -P data http://hapmap.ncbi.nlm.nih.gov/downloads/genotypes/2010-08_phaseII+III/forward/genotypes_chr15_LWK_r28_nr.b36_fwd.txt.gz\n",
      "!wget -P data http://hapmap.ncbi.nlm.nih.gov/downloads/genotypes/2010-08_phaseII+III/forward/genotypes_chr15_MEX_r28_nr.b36_fwd.txt.gz\n",
      "!wget -P data http://hapmap.ncbi.nlm.nih.gov/downloads/genotypes/2010-08_phaseII+III/forward/genotypes_chr15_MKK_r28_nr.b36_fwd.txt.gz\n",
      "!wget -P data http://hapmap.ncbi.nlm.nih.gov/downloads/genotypes/2010-08_phaseII+III/forward/genotypes_chr15_TSI_r28_nr.b36_fwd.txt.gz\n",
      "!wget -P data http://hapmap.ncbi.nlm.nih.gov/downloads/genotypes/2010-08_phaseII+III/forward/genotypes_chr15_YRI_r28_nr.b36_fwd.txt.gz\n",
      "!for f in data/*.gz; do gunzip $f; done"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!head -n 3 data/genotypes_chr15_ASW_r28_nr.b36_fwd.txt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### c) Data preprocessing with `pandas` and `scikit-learn`\n",
      "\n",
      "[TODO]: Explain pandas. Close to Numpy in terms of interface."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "\n",
      "def load_genotypes(filename, population):\n",
      "    df = pd.read_csv(filename, sep=\" \", index_col=0)\n",
      "\n",
      "    # Map genotypes to 0-1-2 values\n",
      "    mask = df.alleles == 'A/C'\n",
      "    df[mask] = df[mask].replace(to_replace=['AA', 'AC', 'CC'], \n",
      "                                value=[0, 1, 2])\n",
      "    mask = df.alleles == 'A/G'\n",
      "    df[mask] = df[mask].replace(to_replace=['AA', 'AG', 'GG'], \n",
      "                                value=[0, 1, 2])\n",
      "    mask = df.alleles == 'A/T'\n",
      "    df[mask] = df[mask].replace(to_replace=['AA', 'AT', 'TT'], \n",
      "                                value=[0, 1, 2])\n",
      "    mask = df.alleles == 'C/G'\n",
      "    df[mask] = df[mask].replace(to_replace=['CC', 'CG', 'GG'], \n",
      "                                value=[0, 1, 2])\n",
      "    mask = df.alleles == 'C/T'\n",
      "    df[mask] = df[mask].replace(to_replace=['CC', 'CT', 'TT'], \n",
      "                                value=[0, 1, 2])\n",
      "    mask = df.alleles == 'G/T'\n",
      "    df[mask] = df[mask].replace(to_replace=['GG', 'GT', 'TT'], \n",
      "                                value=[0, 1, 2])\n",
      "    \n",
      "    # Replace missing value placeholders\n",
      "    df.replace(to_replace='NN', value=-1, inplace=True)\n",
      "\n",
      "    # Copy data into a new frame\n",
      "    individuals = df.columns[10:]\n",
      "    snps = df.index\n",
      "\n",
      "    new_df = pd.DataFrame(index=individuals)\n",
      "    new_df[\"population\"] = population\n",
      "    new_df[snps] = df[individuals].T\n",
      "    new_df = new_df.convert_objects()\n",
      "\n",
      "    # Drop SNPs corresponding to deletions/insertions\n",
      "    new_df = new_df.drop(new_df.columns[new_df.dtypes == \"object\"][1:], axis=1)\n",
      "\n",
      "    return new_df\n",
      "\n",
      "# Concatenate all files\n",
      "populations = [\"ASW\", \"CEU\", \"CHB\", \"CHD\", \"JIH\", \"JPT\", \n",
      "               \"LWK\", \"MEX\", \"MKK\", \"TSI\", \"YRI\"]\n",
      "\n",
      "df = pd.concat([load_genotypes(\"genotypes_chr15_%s_r28_nr.b36_fwd.txt\" % population, \n",
      "                               population) for population in populations], \n",
      "               join=\"inner\")\n",
      "\n",
      "# Convert to Numpy arrays\n",
      "snps = df.columns[1:]\n",
      "X = df[snps].values.astype(np.int8)\n",
      "y = df[\"population\"].values.astype(np.str)\n",
      "\n",
      "# Warning! This takes long..."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = np.load(\"data/chr15-all-numpy.npz\")\n",
      "snps = data[\"snps\"]\n",
      "X = data[\"X\"]\n",
      "y = data[\"y\"]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"SNPs =\", snps[:10]\n",
      "print \"X =\", X\n",
      "print \"X.shape =\", X.shape\n",
      "print \"y =\", y"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Limit to 2000 SNPs\n",
      "snps = snps[:2000]\n",
      "X = X[:, :2000]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.preprocessing import Imputer\n",
      "\n",
      "imputer = Imputer(missing_values=-1, strategy=\"most_frequent\")\n",
      "X = imputer.fit_transform(X)\n",
      "\n",
      "print np.unique(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### d) Visualize the data with `matplotlib` \n",
      "\n",
      "[TODO]: comments on matplotlib, PCA, etc"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.decomposition import RandomizedPCA\n",
      "Xp = RandomizedPCA(n_components=2).fit_transform(X)\n",
      "\n",
      "#from sklearn.manifold import Isomap\n",
      "#Xp = Isomap(n_components=2).fit_transform(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "populations = np.unique(y)\n",
      "colors = plt.get_cmap(\"hsv\")\n",
      "\n",
      "for i, p in enumerate(populations):\n",
      "    mask = (y == p)\n",
      "    plt.scatter(Xp[mask, 0], Xp[mask, 1], \n",
      "                c=colors(1. * i / 11), label=p)\n",
      "    \n",
      "plt.legend()\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[TODO]: comments on clusters with respect to populations"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### e) Learn from the data with `sklearn`"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
      "\n",
      "clf = RandomForestClassifier(n_estimators=1000, n_jobs=2)\n",
      "clf.fit(X_train, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf.score(X_test, y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics import confusion_matrix\n",
      "\n",
      "cm = confusion_matrix(y_test, clf.predict(X_test), labels=populations)\n",
      "\n",
      "print \" \" * 4,\n",
      "for label in populations:\n",
      "    print \" %s\" % label,\n",
      "print\n",
      "\n",
      "for i, label1 in enumerate(populations):\n",
      "    print label1,\n",
      "    for j, label2 in enumerate(populations):\n",
      "        print \"%4d\" % cm[i, j],\n",
      "    print"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "importances = clf.feature_importances_\n",
      "\n",
      "# Plot importances\n",
      "plt.bar(range(len(snps)), importances)\n",
      "plt.show()\n",
      "\n",
      "# Top 10 SNPs\n",
      "indices = np.argsort(importances)[::-1]\n",
      "\n",
      "for i in range(10):\n",
      "    print snps[indices[i]], importances[indices[i]]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}